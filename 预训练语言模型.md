# 预训练语言模型

------



## 预训练

* 定义
	* 通过一个已经训练好的模型A，去完成一个小数据量的任务B（使用了A的浅层参数），局限性是两个任务需要极其相似 (Transformer和BERT解决了)

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20231122001529623.png" alt="image-20231122001529623" style="zoom: 50%;" />

* 为什么需要预训练
	* 很多项目没有大量的数据支持，通过用大数据预训练过的模型可以节省时间提高精确度
	* 可以用类似的已经用大数据训练好的模型，比如要训练分辨猫狗的模型，手头只有一百张猫狗的图片，可以先用大数据训练分辨鸡鸭的模型，浅层上是通用的（横竖撇奈）

* 使用方法
	* 冻结（浅层参数不变，用的较少）
	* ==微调==（浅层参数会跟着任务B训练而改变）

## 语言模型

* 一般有两类问题
  * ==P(“判断这个词的词性”)，P(“判断这个词的磁性”)，判断哪个情况概率大 （1）==
  * ==判断这个词的`__` （2）==

### 统计语言模型

* 用统计的方法去解决上述两个问题

* “判断这个词的词性”这句话可分为 “判断”，“这个”， “词”，“的”，“词性”这几个词，这句话是有序列的

* 用一个条件概率的链式法则（概率论去求），通过这个法则可以求出每一个词出现的概率，然后连乘，就是这句话出现的概率

* $P(w_1,w_2,\cdots,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)\cdots(w_n|w_1,w_2,\cdots,w_{n-1})=\prod_iP(w_i|w_1,w_2,\cdots,w_{i-1})$

* 对于问题（2），常见公式是:  

  ==$P(w_{next} |判断,这个,词,的)=count(w_{next} |判断,这个,词,的) / count(判断,这个,词,的)$ （3）==

  基本方法是把词库V里的每个词都进行（3）的运算，（3）的运算如下图。由于词库的词量非常多（P(w_next |“判断”，“这个”， “词”，“的”， ... , ....)），有了n-grams统计语言模型。

  * n-grams统计语言模型
    * P(w_next |“这个”， “词”，“的”)这样只取出预测词的前三个词进行计算就是三元统计语言模型， P(w_next | “词”，“的”) 这样只取出预测词的前两个词进行计算就是二元统计语言模型

* 平滑策略

  * 防止出现未出现单词，分母为0的情况
  * $P(w_i|w_i-1) = (count(w_{i-1},w_i)+1) / (count(w_{i-1}) + |V|)$

### 神经网络统计语言模型

* 用神经网络的方法去完成两个语言模型任务[(1)(2)](#语言模型)

  * 独热编码 （one-hot coding）

    ![](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20231122015529263.png)

    * 局限性：余弦相似度计算为0，显示词与词之间无关系，但事实上有关系，因此提出词向量

* 由于统计语言模型计算量过大，因此采用神经网络语言模型

  <img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20231122015900910.png" alt="image-20231122015900910"  />

  

## 注意力机制学习

* 为什么需要注意力机制
  * 数据种类众多，对于重要数据我们需要使用，对于不重要数据我们不想使用，对于传统模型（CNN，LSTM），很难决定什么数据重要

* 科学家发现如果给人这张图，会重点关注红色区域，即人物先看脸，文章看标题，段落看开头，文末看落款等，这些区域可能包含更多信息。

  <img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20231122020930228.png" alt="image-20231122020930228" style="zoom: 50%;" />

* 注意力机制流程图 ，Q和k的内积==$F(Q,K)$==得到重要度（相似度）==$s_1,s_2,\cdots, s_n$==，softmax归一化(==$softmax(s_1,s_2,\cdots, s_n)$==) 得到多个概率==$a_1,a_2,\cdots, a_n$==，再乘以对应的value，即

  ==$(a_1,a_2,\cdots, a_n) * (v_1,v_2,\cdots, v_n) = (a_1*v_1 + a_2*a_2 + \cdots + a_n * v_n)$==; 

  需要注意的是这里的==$(s_1,s_2,\cdots, s_n)$==是由==$(F(Q_1,K_1),F(Q_2,K_2),\cdots, F(Q_n,K_n)) / \sqrt(d_k)$==得到，其中==$d_k = d_(model) / h$==, 分子是总的查询维度，分母是注意力头总数，每个注意力头的查询的维度可能不同，但是在同一个头内是相同的。

  <img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20231122022518403.png" alt="image-20231122022518403" style="zoom: 50%;" />

* 假如我（查询对象Q），这张图（被查询对象k）我会判断哪些哦电脑关系对我而言更重要，（去计算Q和K里的事物重要度）,重要度计算其实就是通过点乘的方法计算Q和K里每一个事物的相似度，做一层$Softmax(s_1,s_2,\cdots, s_n)$

  * $Softmax$

    ![](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20231122022634920.png)

    * 主要作用是将输入向量的元素映射为概率分布，确保他们的总和为1
    * 性质
      * 正值输出：Softmax函数的输出始终为正值，因为它涉及指数运算
      * 概率分布：输出向量的元素是归一化的，因为他们表示了输入向量中每个元素对应的类别的概率
      * 敏感块：Softmax函数对输入中较大的值具有相对较高的敏感性，因为指数函数的增长速度取决于输入的大小
    * 用途
      * 在深度学习中，Softmax函数通常用于神经网络的输出层，将网络的原始输出转换为类别概率分布。例如，在图像分类任务中，神经网络可以输出一组得分，Softmax函数将这些得分转换为每个类别的概率。模型最终选择概率最高的类别作为预测结果。Softmax的使用有助于训练和解释模型，因为输出可以解释为属于每个类别的概率。

​	

​	
